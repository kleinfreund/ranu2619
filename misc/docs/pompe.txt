Kapitel 1
Einleitung
Unsere Zeit wird häufig als Informationszeitalter charakterisiert, ist doch der Begriff der Information
aus unserem Alltag nicht mehr wegzudenken. Täglich dringen über die modernen
Kommunikationsmedien Presse, Radio Fernsehen und Computernetze eine Fülle an Nachrichten
auf uns ein, die ein einzelner nicht verarbeiten kann. Digitale Rechenanlagen verarbeiten
sekundenschnell Millionen und Abermillionen von Informationseinheiten. Mit der Geschwindigkeit
des Lichtes werden Informationen rund um die Erde gesandt und durchlaufen die Weiten
des Universums.
Der Menschheit größte technische Anlage bildet das Telefonnetz — eine Einrichtung, die
ausschließlich zur Übertragung von Informationen erdacht wurde. Mit ihr sind nahezu zehn Prozent
aller Menschen miteinander verbunden, heute noch vorrangig durch die Laut– und Schriftsprache,
aber mit der weltweiten Entwicklung des dienstintegrierten Digitalnetzes (ISDN) in
absehbarer Zeit auch zunehmend über Bilder. Tragen weltweite Computernetze auch teilweise
noch recht anarchische Züge, so sind sie doch bereits fester Bestandteil der heutigen Kommunikationstechnik,
die noch vor einem Jahrhundert allzu phantastisch schien. Der technische
Daten– bzw. Informationsaustausch macht heute einen qualitativen Wandel durch, indem er
weniger zentralistisch wird, dafür aber immer stärker sich selbst organisierende, disperse Strukturen
zeigt. Die Unidirektionalität des Austausches etwa beim klassischen Radioempfang wird
zunehmend abgelöst durch die Bi- oder gar Multidirektionalität im World Wide Web (WWW)
bzw. Internet. Die klassischen Kommunikationsmedien werden hier eingebettet. Die Entwicklung
des WWW zeigt erstaunliche Ähnlichkeiten etwa zur Vernetzung natürlicher neuronaler
Netze. Pespektivisch werden die rund 1010 Menschen via WWW in Verbindung stehen, dies ist
56 1. Einleitung
von der gleichen Größenordnung wie die rund 1010 Neuronen im Nervensystem eines einzelnen
Menschen. Hier bilden sie ein komplexes neuronales System, eine neue Qualität, die sich über
die Daseinsweise eines einzelnen Neurons erhebt. Im WWW vernetzen sich Menschen zu eine
Art „Überorganismus“, dessen Konturen heute schon zu erahnen sind. Auch im Mikrokosmos
der Elementarteilchen lassen sich solch neue Entitäten finden, die sich aus der Wechselwirkung
(dem Informationsaustausch) elementarer Einheiten ergeben, etwa dann, wenn unter 1K abgekühlte
Elektronen in einem starken Magnetfeld zusammenwirken und gewisse Quasiteilchen
bilden.
Das Wesen des Zusammenwirkens elementarer Objekte in komplexen Systemen besteht weniger
im Energie– als vielmehr im Informationsaustausch, wenngleich sich beide wechselseitig
bedingen. Dieser Standpunkt wird in immer stärkeren Maße von jenen eingenommen, die komplexe
Systeme erforschen Einen allgemeinverständlichen und aktuellen Überblick hierzu vermittelt
z. B. Mainzer [11]. Die sich zu erwartende Renaissance der Informationstheorie drückte
Wheeler [20] wie folgt aus:
„Tomorrow we will have learned to understand and express all of physics in the language of
information.“1
Der Begriff der Information ist Bestandteil unserer Umgangssprache. Wird er etwa im Sinne
von Nachricht, Auskunft oder Belehrung gebraucht, so findet man, daß Informationen seit
jeher unter Menschen ausgetauscht wurden – mittels Gesten, Lauten, Rauch– und Lichtzeichen
u.v.a.m. Der immer effektivere, schnellere und weiterreichende Informationsaustausch stellt ein
wesentliches Moment der menschlichen Evolution dar. So hatte beispielsweise die Erfindung der
Schrift zur Speicherung oder Übermittlung von Sprachinformationen weitreichende Konsequenzen.
Aber auch die Werke der bildenden Kunst oder Musik haben eine ihnen eigene Sprache,
ihr spezifisches Material zur Speicherung und Übermittlung von Informationen, wie überhaupt
jedes gegenständliche Wirken des Menschen charakteristische Spuren und somit Informationen
für die Nachkommenden hinterläßt.
Der Begriff der Information bzw. des Informationsaustausches ist nicht an zivilisatorische
Entwicklungen gebunden, wenngleich er uns gerade mit Hinblick hierauf in den folgenden Vorlesungen
interessieren wird. Vielmehr werden Informationen in der gesamten Natur ausgetauscht
1
In freier Übersetzung: „Eines Tages werden wir alle Physik aus der Sicht der Information(stheorie) verstehen
und ausdrücken. “1. Einleitung 7
und verarbeitet, in Flora, Fauna und der unbelebten Natur, allerdings auf sehr unterschiedlichem
Niveau. Letztlich ist Informationsaustausch Ausdruck der Wechselwirkung der Materie.
Über Wechselwirkungen hält sich die Materie in Bewegung, tauschen Bereiche der objektiven
Realität Informationen aus und verändern dabei ihren Zustand, wodurch die erhaltene Information
„konserviert“ wird. Sie kann dann von jenen abgerufen werden, die es verstehen, sie zu
lesen und zu interpretieren.
So berichten uns Mondkrater vom Zusammenprall mit einst vagabundierenden Himmelskörpern.
Lebewesen wachsen nach den in der DNS–Doppelhelix gespeicherten Informationen.
Das wohl beeindruckenste Beispiel hierfür hat die Natur im Menschen hervorgebracht, der die
ihm über die Sinnesorgane zugeführten Informationen durch komplexe bio– und physikochemische
Operationen codiert (verschlüsselt), speichert, abruft, decodiert — kurzum: verarbeitet.
Dabei darf jedoch nicht übersehen werden, daß die Wirkung einer Information auf den Empfänger
wesentlich von seinem Systemzustand abhängt, von seiner Fähigkeit zur Interpretation
(Decodierung) der Informationen und seinen Möglichkeiten der Reaktion in Form der eigenen
Zustandsänderung sowie der Aussendung von Informationen an die Umwelt.
Die erwähnten Beispiele machen deutlich, daß die Begriffe der Information und des Informationsaustausches
objektive Kategorien sind, also des Menschen und seiner Reflexionen nicht
bedürfen. Sie sind der Daseinsweise der Natur inhärent. Insbesondere existieren sie unabhängig
von der modernen Kommunikationstechnik. Allerdings wurde der Begriff der Information erst
mit der Entwicklung dieser Kommunikationstechnik in unserem Jahrhundert zur wissenschaftlichen
Kategorie, als es dem amerikanischen Mathematiker und Nachrichtentechniker Claude
E. Shannon Ende der vierziger Jahre gelang, ein Maß für die Information zu begründen. Dadurch
wurde etwas meßbar, was zuvor als unmeßbar galt. Shannon stellte seine Ergebnisse
zunächst auf einer Tagung in New York City im März 1948 vor. Im gleichem Jahr erschien seine
berühmte Arbeit A mathematical theory of communication im Bell System Technical Journal.
Eine weitere, in diesem Zusammenhang wichtige Erfindung fällt in den Juli desselben Jahres
— die öffentliche Ankündigung der technischen Realisierung eines Festkörper–Verstärkers, des
sogenannten Transistors. Seine Erfinder um den Amerikaner Shockley entwickelten diesen
Transistor in den Bell Telephone Laboratorien, wo auch Shannon angestellt war. Im Jahre
1948 erschien aber auch das Buch des Amerikaners Norbert Wiener mit dem Titel Cybernetics
or control and communication in the animal and the machine. Damit begründete Wie-8 1. Einleitung
ner die Kybernetik als die Wissenschaft von den abstrakten dynamischen Systemen, die über
Informationsverarbeitung, Selbstregulation und –organisation verfügen. Mit diesen Theorien,
Entdeckungen und Erfindungen gilt das Jahr 1948 heute als das Geburtsjahr der Informationstheorie
und der modernen Kommunikationstechnik, als der Beginn des technischen Informationszeitalters.
Selbstverständlich hatten die erwähnten Fortschritte ihre Vorläufer und Begleiter.
So sind bezüglich der Shannonschen Informationstheorie beispielsweise Namen und Zeiten wie
Gabor (1946), Kotelnikov, Nyquist (1924/28) und Hartley (1928) zu nennen.
Nimmt ein System aus seiner Umwelt Informationen auf, so führt dies infolge der Informationsspeicherung
zur Erhöhung des Ordnungszustandes in diesem System. Damit verringert
sich seine thermodynamische Entropie. Folglich erwarten wir eine grundsätzliche Beziehung
zwischen dem nachrichtentechnischen Informationsbegriff und der thermodynamischen Entropie,
welche Clausius im Jahre 1865 einführte. Solche Zusammenhänge konnten tatsächlich
aufgezeigt werden, vor allem durch die Untersuchungen von Boltzmann (1872/77), Szilard
(1929), Wiener (1949) und Brillouin (1956).
Charakteristisch für Shannons Theorie ist, daß sie keine Theorie des Nachrichtenmittels
ist, sondern die Theorie der Nachricht selbst. Damit ist sie allgemein genug, um in den unterschiedlichen
Wissenschaften, von der Physik über die Biologie bis hin zur Psychologie, den
Erziehungs– und Kunstwissenschaften u.a. Bedeutung erlangen zu können. Die Information ist
damit eine der allgemeinsten wissenschaftlichen Begriffe überhaupt.
Versucht man eine Klassifizierung der großen Aufgabenstellungen, welchen sich die Physik
und die darauf aufbauenden technischen Disziplinen vorrangig widmen, so findet man Komplexe,
welche sich um die Begriffe Stoff, Energie und Information ranken. Zum ersten Begriff
gehören die Materialwissenschaften. Hier geht es darum, Materialien mit gewissen ausgezeichneten
Eigenschaften zu erzeugen, die der Oberflächenveredlung dienen, der effektiven Leitung
elektrischer Ströme z. B. mit Supraleitern u.v.a.m. Die zweite Kategorie ist mit den Bemühungen
verknüpft, umweltverträgliche und kostengünstige Energieträger großtechnisch zu nutzen.
Hier gehören Begriffsfelder her wie die Kernfusion und die Photovoltaik. Der Informationsbegriff
eröffnet eine in seiner Bedeutung ebenbürtige dritte Sparte, wenngleich generell anzumerken
ist, daß sich diese Gebiete gegenseitig bedingen und durchdringen.
Im Unterschied zu Hartley (1928) gebrauchte Shannon (1948) den Begriff Information
nicht. Vielmehr sprach er von der Kommunikation (communication). Nach allgemeiner Auffas-1.0. Gegenstand der Informationstheorie 9
sung geht der Gebrauch des Begriffes Information im hier interessierenden Zusammenhang auf
Wiener zurück. In seinem bereits erwähnten Buch aus dem Jahre 1948 schreibt er
„Information is information not matter or energy.“ 2
Die Dreiheit von Stoff, Energie und Information haben wir bereits hervorgehoben. Haben wir ein
stoffliches Gebilde mit einem bestimmten Energieinhalt, so beschreibt die Information (Entropie)
gewissermaßen seinen Ordnungszustand.
In den 50er und 60er Jahren gab es wesentliche Weiterentwicklungen der Informationstheorie
mit breiten Anwendungen in den verschiedensten Wissenschaftsdisziplinen. Jedoch wurde die
Bedeutung dieser Theorie dabei teilweise auch überschätzt. Aber bis in die Gegenwart hinein
spielen Begriffe aus der Informationstheorie eine teilweise zentrale Rolle in sich neu entwickelnden
Theorien, wie jener dynamischer Systeme (Chaostheorie).
Gegenstand der Informationstheorie
Die Informationstheorie beschäftigt sich mit theoretischen Problemen bei der Aufbewahrung
(Speicherung), Verschlüsselung (Codierung) und Übertragung von Informationen. Shannons
Betrachtungen setzen ein allgemeines Übertragungsschema voraus, wie es in der Abbildung 1.1
zu sehen ist. Jede Schnittstelle in diesem Schema kann ihrerseits als ein Paar aus Informationsquelle
und –senke aufgefaßt werden. Quelle und Senke der Information heißen auch Sender
bzw. Empfänger.
Träger der Information sind Signale, die in der Nachrichtentechnik häufig aus einer Folge
verschiedener Pegel eines elektrischen Potentials bestehen. Diese Signale werden zunächst
geeignet codiert, was man Quellencodierung nennt. Die Informationstheorie macht Aussagen
darüber, was man unter einer optimalen Codierung einer Informationsquelle zu verstehen hat
und wie man dieses Optimum erreicht. Eine Codierung ist optimal, wenn sie zu einer möglichst
kompakten (kurzen) Darstellung führt. Denken wir z. B. an Dateien in einem Personalcomputer.
Um hier mit den Speicherressourcen möglichst sparsam zu sein, werden sogenannte Packer
verwendet, welche die Dateilängen verkürzen. Dabei dürfen selbstverständlich keine Informationen
verloren gehen — mit den Entpackern müssen wir jederzeit den Originalfile eindeutig
rekonstruieren können, ebenso wie unsere Hand einen Schwamm zunächst auf kleinem Raum
zusammendrückt und dann wieder frei gibt, worauf sich die usprünglichen Konturen zeigen.
2
In freier Übersetzung: „Information ist etwas Eigenständiges, das weder Stoff noch Energie ist.“10 1. Einleitung
Informations–Quelle
Quellen–Codierer
Kanal–Codierer
Kanal Störung
Kanal–Decodierer
Quellen–Decodierer
Informations–Senke
Abb. 1.1: Allgemeines Schema zur Informationsübertragung
Der Quellencodierung folgt die sogenannte Kanalcodierung. Sie ist notwendig, weil in realen
Systemen die Übertragung immer mehr oder weniger stark gestört wird, besonders dann,
wenn hochfrequente Signale übertragen werden. Deshalb kann in der Zeiteinheit nicht beliebig
viel Information durch einen realen Nachrichtenkanal gelangen, ebenso, wie durch ein Rohr in
der Sekunde nicht beliebig viel Wasser fließen kann. Es ist eine Grundaussage der Informationstheorie,
daß trotz gewisser Störungen prinzipiell immer eine bestimmte Informationsmenge
fehlerfrei übertragen werden kann. Der maximal mögliche (fehlerfreie) Informationsfluß ist die
Kanalkapazität. Soll sie gut ausgenutzt werden, so muß durch eine geeignete Codierung das
Signal an den Kanal angepaßt werden. Die Informationstheorie macht Aussagen darüber, wie
dies zu erfolgen hat.
Generell sei jedoch angemerkt, daß die Informationstheorie kaum konstruktiv ist. Die grundlegenden
Algorithmen zur optimalen Codierung können nur in besonders einfachen Fällen in
der Praxis umgesetzt werden. In ihren wesentlichen ursprünglichen Aussagen gibt sie eher prinzipielle
Grenzen für das praktisch Machbare an. Der Ingenieur muß dann seine ganze Findigkeit
einsetzen, um mit der aufgebauten Apparatur diesen Grenzen möglichst nahe zu kommen. An
diesen theoretisch abschätzbaren Grenzen mißt sich dann der Wert der Apparatur.1.0. Etymologie 11
Aus der Informations– ist die Codierungstheorie hervorgegangen, welche wegen ihrer praktischen
Bedeutung zuweilen als eigenständige Disziplin aufgefaßt wird. Hier geht es u. a. darum,
Codes mit „kontrollierter Redundanz“ zu konstruieren. Denken wir dazu beispielsweise an unsere
natürliche Schriftsprache, mit der dieses Buch geschriben ist. Der Sinn des obigen Satzes
ist offenbar nicht verfälscht, obwohl im vorletzten Wort ein Buchstabe fehlt. Dieser Buchstabe
trägt also zu einer scheinbar unnötigen Weitschweifigkeit (Redundanz) des geschriebenen Textes
bei. Tatsächlich kann man viele weitere Beispiele für eine derartige Weitschweifigkeit finden,
wenngleich in einigen Fällen durchaus schon ein einzelner fehlender oder falscher Buchstabe
sinnentstellend sein kann: Das Kind ißt eine Möhre. — Das Kind ist eine Göre. Entsprechende
Beispiele finden sich auch in der mündlichen Sprache. Tatsächlich erfüllt die Weitschweifigkeit
aber eine wichtige Funktion: durch sie wird die Informationsübertragung bei zufälligen Störungen
sicherer. Wählt man geschickte Codierungen, so kann man mit einer möglichst geringen
Weitschweifigkeit einen möglichst großen Gewinn an Übertragungssicherheit erzielen.
Die mathematischen Grundlagen der Informationstheorie bilden Wahrscheinlichkeitrechnung
und Statistik. Sie werden in dieser Vorlesung zumeist auf einem elementaren Niveau
benötigt.
Etymologie des Informationsbegriffes
Die Worte informieren und Information wurden im 15. bzw. 16. Jahrhundert dem lateinischen
Verb in–formare bzw. dem Substantiv informatio entlehnt. Das Verb wurde in dessen übertragener
Bedeutung „durch Unterweisung bilden, unterrichten“ gebraucht. Eigentlich bedeutet in–
formare „eine Gestalt geben“, „formen“, „bilden“ oder auch „in eine Form geben“. Das Substantiv
Information wurde zunächst im Sinne von „Nachricht, Auskunft, Belehrung“ gebraucht. In der
Zeit des Humanismus und der Renaissance trat dann die Bedeutung „Bildung (Unterweisung)
durch den Dorfschullehrer (Informator)“ in den Vordergrund. Bis weit in das 19. Jahrhundert
war die Bezeichnung „Informator“ für den Hauslehrer geläufig. Dann ging dieser Sinn mehr und
mehr verloren. Schließlich blieb das Substantiv bis in unsere Tage im Sinne von „Auskunft,
Mitteilung, Nachricht, Neuigkeit“ in der Alltagssprache erhalten. Der Gebrauch der Informationsbegriffes
in den Wissenschaften setzt allerdings seine strenge formalisierte Definition voraus.
Dabei sollte sich unser intuitiver, in der natürlichen Sprache entwickelter Informationsbegriff
in der Formalisierung wiederfinden. Dies wird zumindest in einigen Aspekten auch tatsächlich12 1. Einleitung
erreicht. Nur deshalb konnten wir bislang nahezu unmißverständlich den Informationsbegriff
gebrauchen.
Nach genauer Analyse unseres intuitiven Informationsbegriffes stellt sich heraus, daß man
ihn unter drei verschiedenen Gesichtspunkten sehen kann:
1. syntaktisch–statistisch,
2. semantisch und
3. pragmatischer Aspekt.
Der erste Gesichtspunkt berücksichtigt nur die Wahrscheinlichkeiten, mit denen die Zeichen
aus einer Informationsquelle gesandt werden. Hingegen sagt der zweite Aspekt etwas über die
Bedeutung aus, welche die Zeichen für den Empfänger haben. Schließlich berücksichtigt der
dritte Aspekt die Wirkung der Zeichen auf den Empfänger.
Wegen der Komplexität unseres intuitiven Informationsbegriffes kann es nicht verwundern,
daß ein formalisierter Informationsbegriff immer nur gewissen Teilaspekten des intuitiven
Begriffes Rechnung tragen kann. So trägt der Shannonsche Informationsbegriff nur dem
syntaktisch–statistischen Aspekt Rechnung. Semantische Aspekte werden nicht erfaßt, sie sind
für den klassischen Nachtrichteningenieur unwichtig. Allerdings gab es Bemühungen, allgemeinere
Informationsmaße zu begründen. Wir werden in einem späteren Abschnitt auf derartige
Verallgemeinerungen eingehen.
